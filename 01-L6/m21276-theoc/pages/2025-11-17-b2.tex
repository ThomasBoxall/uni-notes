\taughtsession{Lecture}{B2: Computability and Equivalent Models P2. 2}{2025-11-17}{15:00}{Janka}

10 minute doom-scroll-of-cat-photos break later, we're back at it for Part 2 of this 2 parter.

This lecture picks up directly from the last - starting with...

\section{Model 5: Recursive Functions}
As we have seen in the second year Discrete Maths module - the concept of a \textit{function} is fundamental to much of mathematics.

In this topic, we consider functions whose arguments and values are natural numbers. The basic characteristic of a \textit{computable function} is that there must be a finite procedure (known as an algorithm) telling the `computer' how to compute the function. Expressions can be one way to define this, such as $f(n) = n^3 + 1$ as a clear recipe is given; however for more vague functions (such as the busy beaver), there isn't a definitive recipe for them. 

Church, Kleene \& Rosser created a formal definition of a class of functions whose values could be calculated by recursion, which are known as the \textit{partial recursive functions}.

We believe that functions of the following form are computable:
\[f(x)=0, \quad g(x)=x+1, \quad H(x,y,z)=x\]

In some fun news - all we need to construct all possible computational functions are basic functions like those above, and some simple combining rules! Unlike for Turing Machines, the description of recursive functions is \textit{inductive}. 

\subsection{The Idea...}
The idea of recursive functions is simple - we break down the target into a series of steps which the computer is able to compute.

If we consider the function $exp(x,y)$ which finds the $y$ exponent of $x$ ($x^y$), we know that
\begin{align*}
    x^0 &= 1\\
    x^1 &= x\\
    x^2 &= x \cdot x\\
    x^3 &= x \cdot x \cdot x
\end{align*}

We can see a sort of generalisation forming here:
\begin{align*}
    x^y &= x \cdot x \cdots x && (y \textrm{ occurrences of } x)\\
    x^{y+1} &= x \cdot x \cdots x && (y+1 \textrm{ occurrences of } x)\\
    &= x \cdot x^y && \ 
\end{align*}

From this we can see two ``rewriting rules'' emerge which are enough to define the $exp$ function:
\begin{align*}
    x^0 &= 1\\
    x^{y+1} & x \cdot x^y
\end{align*}

Well this is most exciting - we can see that the rules for $exp$ reduce exponentiation to multiplication. 

If we now consider multiplication:
\begin{align*}
    x \cdot 0 &= 0\\
    x \cdot (y+1) &= x + x \cdot y
\end{align*}

We can see that rules for multiplication reduce the multiplication operation to additions. I spy a pattern here...

We can finally take a look at the rules for addition:
\begin{align*}
    x+0 &= x \\
    x + (y+1) &= (x+y) + 1
\end{align*}

Here we find the simplest, most primitive operation we know about: $succ$, which adds 1. 

From this exercise of inductive decomposition, we've found that primitive recursion is in the spirit of ``computation by rewriting'' definitions of $exp$, $\cdot$ and $+$. If we want to use ``recursion'', it consists of one rule for $y=0$ and one rule for $y>0$ where $y$ acts as a countdown for the number of remaining steps in the computation.

There are five building blocks for primitive recursive functions:
\begin{itemize}
    \item Three basic functions: \textit{successor}, \textit{zero} and \textit{projections}
    \item Two ways of building new primitive functions from old ones: \textit{composition} and \textit{primitive recursion}
\end{itemize}

\subsection{Recursive Function Building Blocks}
The \textit{successor} function we met in the last lecture as part of the simple programming language. Successor takes a value and returns the successor number to it: $succ(x) = x+1$. 

The \textit{zero} function is a simple function. It takes a natural number and returns 0: $zero(x) = 0$. 

The \textit{projection} function takes a natural number as input, $i$ and returns the $i$-th element of the provided tuple of natural numbers: $project_i: \mathbb{N}^k \rightarrow \mathbb{N}$, where
\[project_i(x_1, \ldots, x_k) = x_i, i \in \{1, \ldots, k\}\]

\textit{Composition} replaces the arguments of a function with other functions. If we take $g_1$, $g_2$, \ldots, $g_m$ as functions $\mathbb{N}^k \rightarrow \mathbb{N}$, and $f$ is a function $\mathbb{N}^m \rightarrow \mathbb{N}$, then the function $h: \mathbb{N}^k \rightarrow \mathbb{N}$ is given by:
\[h(x_1, \ldots, x_k) = f(g_1(x_1, \ldots, x_k), \ldots, g_m(x_1, \ldots, x_k))\]
Then the function $h$ is said to arise by composition from $f$, $g_1$, \ldots, $g_m$. 

\begin{example}{Composition}
If we consider the function:
\[h(x) = succ(succ(zero(x)))\]

We can see that this function will always return 2 and is made of three functions: $succ$ twice, and $zero$ once.
\end{example}

\textit{Primitive Recursion} is where a new function is defined in terms of existing functions as follows:
\begin{align*}
    f(x_1, x_2, \ldots, x_n, 0) &= h(x_1, x_2, \ldots, x_n)\\
    f(x_1, x_2, \ldots, x_n, succ(y)) &= g(x_1, x_2, \ldots, x_n, y, f(x_1, x_2, \ldots, x_n, y))
\end{align*}

The first function above uses $y=0$ and as we saw above in our inductive experiments - this is defined in a different way as this is often directly computable. The second function then increases $y$ to the point needed for the equation - the right hand side using $y$ as is and the left hand side using $succ(y)$ to increase it's value by 1. (We'll see an example of this in action shortly).

A function is \textit{primitive recursive} if it can be built up using the base functions and the operations of composition and primitive recursion.  

\begin{example}{Addition as a Primitive Recursive Function}
If we take the function $add(x,y) = x+y$ we can see it's primitive recursiveness.

Firstly, we define addition recursively:
\begin{align*}
    add(x,0) &= x\\
    add(x,succ(y)) &= succ(add(x,y))
\end{align*}
Where the first row is the zero-condition which the computer will always be able to compute, and the second row is the recursive statement which we can use to sum two numbers. 

\begin{align*}
    add(2,2) &= succ(add(2,1))\\
    &= succ(succ(add(2,0)))\\
    &= succ(succ(2))\\
    &= succ(3)\\
    &= 4
\end{align*}

We can see that rewriting the right hand side of the recursive addition definition to use primitive recursive functions:
\begin{align*}
    f(x,0) &= h(x)\\
    f(x, succ(y)) &= g(x,y,f(x,y))
\end{align*}

where:
\begin{align*}
    h(x) &= x = project_1(x)\\
    g(x,y,u) &= succ(u) = succ(project_3(x,y,u))
\end{align*}
\end{example}

Most of the functions normally studied in number theory are primitive recursion. Addition, division, factorial, exponential and the $n$-th prime are all primitive recursion. From this we see that the primitive recursive functions are defined for all non-negative natural numbers (total functions).

From the definition it follows a primitive recursive function is computable by a Turing Machine. However, there are known functions which are not primitive recursive. 

\subsection{Ackermann Function}
The Ackermann Function is a total function which is very fast growing. It grows faster than any primitive recursive function does. Ackermann's function is defined by:
\begin{align*}
    A(0,y) &= y + 1\\
    A(x,0) &= A(x-1, 1)\\
    A(x,y+1) &= A(x-1, A(x,y))
\end{align*}

This definition is given by formula, which is not a primitive recursive formula. However this is not a proof, although this doesn't mean that the formula cannot be massaged into the primitive recursive function.

Ackermann's function is not primitive recursive.

\subsection{Minimisation}
\textit{Minimisation} defines a new function $f$ in terms of a total function $g$ as follows (where $x$ represents any number of arguments). The value $f(x)$ is determined by searching the following sequence for the smallest $y$ such that $g(x,y)=0$. 
\[g(x,0),\ g(x,1),\ g(x,2),\ \ldots\]
If such a $y$ exists then define $f(x)=y$. Otherwise, $f(x)$ is undefined.
\[f(x) = min(y,g(x,y)=0)\]

The algorithm used by $f(x)$ is shown below:
\begin{verbatim}
y = 0;
while (not (g(x,y) = 0)){
    y = y + 1;
}
return y;
\end{verbatim}

\subsection{Recursive Functions: A Summary}
A function is considered to be \textit{partial recursive} if it can be built up using the base functions and the operations of composition, primitive recursion, and minimisation. A function can be computed by a Turing machine if and only if it is partial recursive, which can also be denoted as $\mu$-recursive. 

\begin{example}{Recursive Functions}
\textbf{Ex. 1} $f(x) = min(y,xy=0)$ defines $f(x) = 0$. 

\textbf{Ex. 2} $f(x) = min(y,x+y = 0)$ defines $f(x)=$ `if $x=0$ then 0, else undefined'.
\end{example}

\section{Model 6: Cellular Automata - The Game of Life (Interest Only)}
As we saw towards the end of Part A - one variation of the Turing Machine is to have many tape heads where many cells can be modified in each step. If we take this one step further such that we can modify every cell in each step, all in parallel - we find the basis of \textit{cellular automata}.

We will focus on one particular cellular automata called the \textit{Game of Life} which was devised by Conway, a British mathematician in 1970. Rendell in 2000 and 2010 proved that the Game of Life is equivalent in power to the Universal Turing Machine.

The Game of Life is an infinite two-dimensional orthogonal grid of square cells. Each cell is in one of two states: live or dead. Every cell interacts with it's neighbours which are the cells that are horizontally, vertically, or diagonally adjacent. At each step in time - there are three possible transitions which can occur depending on the number of live neighbours:
\begin{description}
    \item[Death] if the count is less than 2 or greater than 3, the current cell is switched off (dies) 
    \item[Survival] if the count is exactly 2 or the count is exactly 3 and the current cell lives, the current cell is left unchanged (it lives on to the next generation)
    \item[Birth] if the current cell is off (dead) and the count is exactly 3, the current cell is switched on (becomes live)
\end{description}

The initial pattern in the GoL is called the \textit{seed} of the system. The first generation is created by applying the rules above simultaneously to every cell in the seed - births and deaths occur simultaneously in each generation, depending on the neighbours of the previous generation. The rules continue to be applied repeatedly to create further generations.

\begin{extlink}
There are links in the slides on Moodle, as well as on Moodle itself, to simulators for the Game of Life.
\end{extlink}